{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing SageMaker libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.predictor import csv_serializer\n",
    "\n",
    "region = boto3.Session().region_name\n",
    "\n",
    "session = sagemaker.Session()\n",
    "\n",
    "bucket = session.default_bucket()\n",
    "prefix = 'sagemaker/autopilot-churn'\n",
    "\n",
    "role = get_execution_role()\n",
    "\n",
    "sm = boto3.Session().client(service_name='sagemaker',region_name=region)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing other libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import io\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import json\n",
    "from IPython.display import display\n",
    "from time import strftime, gmtime, sleep\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.kaggle.com/c/customer-churn-prediction-2020/data?select=train.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "churn = pd.read_csv('./data/churn.csv')\n",
    "pd.set_option('display.max_columns', 500)\n",
    "churn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload file to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_data = #TODO\n",
    "train_data_s3_path = session.upload_data(path='./data/train.csv', key_prefix=prefix + \"/train\")\n",
    "print('Train data uploaded to: ' + train_data_s3_path)\n",
    "\n",
    "test_data_s3_path = session.upload_data(path='./data/test.csv', key_prefix=prefix + \"/test\")\n",
    "print('Test data uploaded to: ' + test_data_s3_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initiate the AutoPilot (AutoML) training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data_config = [{\n",
    "      'DataSource': {\n",
    "        'S3DataSource': {\n",
    "          'S3DataType': 'S3Prefix',\n",
    "          'S3Uri': 's3://{}/{}/train'.format(bucket,prefix)\n",
    "        }\n",
    "      },\n",
    "      'TargetAttributeName': 'churn'\n",
    "    }\n",
    "  ]\n",
    "\n",
    "output_data_config = {\n",
    "    'S3OutputPath': 's3://{}/{}/output'.format(bucket,prefix)\n",
    "  }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Launch the job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp_suffix = strftime('%d-%H-%M-%S', gmtime())\n",
    "\n",
    "autopilot_job_name = 'autopilot-churn-' + timestamp_suffix\n",
    "print('Job Name: ' + autopilot_job_name)\n",
    "%store autopilot_job_name\n",
    "\n",
    "sm.create_auto_ml_job(AutoMLJobName=autopilot_job_name,\n",
    "                      InputDataConfig=input_data_config,\n",
    "                      OutputDataConfig=output_data_config,\n",
    "                      AutoMLJobConfig={'CompletionCriteria': {'MaxCandidates': 20}\n",
    "                                      },\n",
    "                      RoleArn=role)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tracking job progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('Overall Status - Secondary Status')\n",
    "print('------------------------------')\n",
    "\n",
    "\n",
    "describe_response = sm.describe_auto_ml_job(AutoMLJobName=autopilot_job_name)\n",
    "print (describe_response['AutoMLJobStatus'] + \" - \" + describe_response['AutoMLJobSecondaryStatus'])\n",
    "job_run_status = describe_response['AutoMLJobStatus']\n",
    "    \n",
    "while job_run_status not in ('Failed', 'Completed', 'Stopped'):\n",
    "    describe_response = sm.describe_auto_ml_job(AutoMLJobName=autopilot_job_name)\n",
    "    job_run_status = describe_response['AutoMLJobStatus']\n",
    "    \n",
    "    print (describe_response['AutoMLJobStatus'] + \" - \" + describe_response['AutoMLJobSecondaryStatus'])\n",
    "    sleep(30)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you're using SageMaker Studio, try look around at **Experiments and trials** for visualization of AutoPilot job status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optionally, explore Auto-generated codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(describe_response['AutoMLJobArtifacts']['CandidateDefinitionNotebookLocation'])\n",
    "#print(describe_response['AutoMLJobArtifacts']['DataExplorationNotebookLocation'])\n",
    "\n",
    "#candidate_nbk = describe_response['AutoMLJobArtifacts']['CandidateDefinitionNotebookLocation']\n",
    "#data_explore_nbk = describe_response['AutoMLJobArtifacts']['DataExplorationNotebookLocation']\n",
    "\n",
    "#def split_s3_path(s3_path):\n",
    "#    path_parts=s3_path.replace(\"s3://\",\"\").split(\"/\")\n",
    "#    bucket=path_parts.pop(0)\n",
    "#    key=\"/\".join(path_parts)\n",
    "#    return bucket, key\n",
    "\n",
    "#s3_bucket, candidate_nbk_key = split_s3_path(candidate_nbk)\n",
    "#_, data_explore_nbk_key = split_s3_path(data_explore_nbk)\n",
    "\n",
    "#session.download_data(path='./', bucket=s3_bucket, \n",
    "                                 key_prefix = candidate_nbk_key)\n",
    "\n",
    "#session.download_data(path='./', bucket=s3_bucket, \n",
    "                                 key_prefix = data_explore_nbk_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're playing the wait game, and get the best candidate result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_candidate = sm.describe_auto_ml_job(AutoMLJobName=autopilot_job_name)['BestCandidate']\n",
    "best_candidate_name = best_candidate['CandidateName']\n",
    "print(best_candidate)\n",
    "print('\\n')\n",
    "print(\"CandidateName: \" + best_candidate_name)\n",
    "print(\"FinalAutoMLJobObjectiveMetricName: \" + best_candidate['FinalAutoMLJobObjectiveMetric']['MetricName'])\n",
    "print(\"FinalAutoMLJobObjectiveMetricValue: \" + str(best_candidate['FinalAutoMLJobObjectiveMetric']['Value']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now deploy the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp_suffix = strftime('%d-%H-%M-%S', gmtime())\n",
    "model_name = best_candidate_name + timestamp_suffix + \"-model\"\n",
    "model_arn = sm.create_model(Containers=best_candidate['InferenceContainers'],\n",
    "                            ModelName=model_name,\n",
    "                            ExecutionRoleArn=role)\n",
    "\n",
    "epc_name = best_candidate_name + timestamp_suffix + \"-epc\"\n",
    "ep_config = sm.create_endpoint_config(EndpointConfigName = epc_name,\n",
    "                                      ProductionVariants=[{'InstanceType': 'ml.m5.2xlarge',\n",
    "                                                           'InitialInstanceCount': 1,\n",
    "                                                           'ModelName': model_name,\n",
    "                                                           'VariantName': 'main'}])\n",
    "\n",
    "ep_name = best_candidate_name + timestamp_suffix + \"-ep\"\n",
    "create_endpoint_response = sm.create_endpoint(EndpointName=ep_name,\n",
    "                                              EndpointConfigName=epc_name)\n",
    "\n",
    "print('Endpoint: ' + create_endpoint_response['EndpointArn'])\n",
    "%store create_endpoint_response['EndpointArn']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And optinally, evaluate it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv('./data/test.csv')\n",
    "test_data_inference = test_data.drop('churn', axis=1)\n",
    "\n",
    "if (sagemaker.__version__ < '2'):\n",
    "    from sagemaker.predictor import RealTimePredictor\n",
    "    from sagemaker.content_types import CONTENT_TYPE_CSV\n",
    "    predictor = RealTimePredictor(\n",
    "        endpoint=ep_name,\n",
    "        sagemaker_session=session,\n",
    "        content_type=CONTENT_TYPE_CSV,\n",
    "        accept=CONTENT_TYPE_CSV)\n",
    "\n",
    "    prediction = predictor.predict(test_data_inference.to_csv(sep=',', header=False, index=False)).decode('utf-8')\n",
    "\n",
    "else:\n",
    "    from sagemaker.predictor import Predictor\n",
    "    from sagemaker.serializers import CSVSerializer\n",
    "    from sagemaker.deserializers import CSVDeserializer\n",
    "    predictor = Predictor(\n",
    "        endpoint_name=ep_name,\n",
    "        sagemaker_session=session,\n",
    "        serializer=CSVSerializer(),\n",
    "        deserializer=CSVDeserializer())\n",
    "    \n",
    "    prediction = predictor.predict(test_data_inference.to_csv(sep=',', header=False, index=False))\n",
    "\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "accuracy = accuracy_score(test_data.reset_index()['churn'], prediction_df[0])\n",
    "precision = precision_score(test_data.reset_index()['churn'], prediction_df[0], pos_label='True.')\n",
    "recall = recall_score(test_data.reset_index()['churn'], prediction_df[0], pos_label='True.', average='binary')\n",
    "f1 = f1_score(test_data.reset_index()['churn'], prediction_df[0], pos_label='True.')\n",
    "\n",
    "print('Accuracy: {}'.format(accuracy))\n",
    "print('Precision: {}'.format(precision))\n",
    "print('Recall: {}'.format(recall))\n",
    "print('F1: {}'.format(f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, when you're done, remember to delete the endpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sm.delete_endpoint(EndpointName=ep_name)\n",
    "#sm.delete_endpoint_config(EndpointConfigName=epc_name)\n",
    "#sm.delete_model(ModelName=model_name)\n"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
